{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd4pt-vw0EjZ"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!nohup ollama serve > output.log 2>&1 &\n",
        "!ollama pull phi4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchainhub langchain-community langchain-core ollama faiss-cpu PyMuPDF requests"
      ],
      "metadata": {
        "id": "7TZ1UHz907SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOllama\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2306.15595.pdf\"\n",
        "pdf_path = \"sample.pdf\"\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "    response = requests.get(pdf_url)\n",
        "    with open(pdf_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Load and split the PDF\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Create embeddings and FAISS vector store\n",
        "embedding_func = OllamaEmbeddings(model=\"phi4\")\n",
        "vector_store = FAISS.from_documents(chunks, embedding_func)\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Short-term memory\n",
        "short_term_memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOllama(model=\"phi4\")\n",
        "\n",
        "# long term memory setup\n",
        "# long_term_vector_store = FAISS.from_texts([], embedding_func)\n",
        "# long_term_memory = VectorStoreRetrieverMemory(\n",
        "#     retriever=long_term_vector_store.as_retriever(),\n",
        "#     memory_key=\"history\",\n",
        "#     input_key=\"question\"\n",
        "# )\n",
        "\n",
        "# # Step 6: Fact extractor\n",
        "# fact_prompt = PromptTemplate(\n",
        "#     input_variables=[\"question\", \"answer\"],\n",
        "#     template=\"\"\"\n",
        "# You are a memory manager. A user asked a question and got an answer.\n",
        "# Your task is to extract any important factual info about the user or knowledge\n",
        "# that should be remembered for future conversations.\n",
        "\n",
        "# User Question: {question}\n",
        "# Bot Answer: {answer}\n",
        "\n",
        "# Only extract facts worth storing (e.g., preferences, facts, interests, tasks).\n",
        "# If nothing useful, return \"None\".\n",
        "\n",
        "# Important Fact to Store:\n",
        "# \"\"\"\n",
        "# )\n",
        "\n",
        "# fact_extractor = ChatOllama(model=\"phi4\")\n",
        "\n",
        "# Custom prompt to include conversation history\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\", \"context\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful AI assistant. Use the following context and chat history to answer the question. Remember details about the user as well, and answer from them if asked.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# QA Chain with custom prompt and memory\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=short_term_memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "# Chat function\n",
        "def ask_chat_bot(user_query):\n",
        "    response = qa_chain.invoke({\"question\": user_query})\n",
        "    return response['answer']\n",
        "\n",
        "# change in the function if I wanna add long term memory\n",
        "# def ask_chat_bot(user_query):\n",
        "#     response = qa_chain.run(user_query)\n",
        "\n",
        "#     # Extract factual info from the conversation\n",
        "#     fact_input = fact_prompt.format(question=user_query, answer=response)\n",
        "#     extracted_fact = fact_extractor.invoke(fact_input).content.strip()\n",
        "\n",
        "#     if extracted_fact and extracted_fact.lower() != \"none\":\n",
        "#         long_term_memory.save_context({\"question\": user_query}, {\"output\": extracted_fact})\n",
        "\n",
        "#     return response\n",
        "\n",
        "# Command line chat interface\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ¤– Hello! Ask me anything based on the PDF or general info. Type 'quit' or 'exit' to close the chatbot.\")\n",
        "    while True:\n",
        "        user_query = input(\"\\nðŸ§‘ You: \")\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ðŸ‘‹ Bye!\")\n",
        "            break\n",
        "        bot_reply = ask_chat_bot(user_query)\n",
        "        print(\"ðŸ¤– Bot:\", bot_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKw0dTMm1MBO",
        "outputId": "6bd8a187-de1b-4ebf-ad60-a03d5fc71e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-39b093c21ff0>:28: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
            "  embedding_func = OllamaEmbeddings(model=\"phi4\")\n",
            "<ipython-input-3-39b093c21ff0>:33: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  short_term_memory = ConversationBufferMemory(\n",
            "<ipython-input-3-39b093c21ff0>:39: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
            "  llm = ChatOllama(model=\"phi4\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Hello! Ask me anything based on the PDF or general info. Type 'quit' or 'exit' to close the chatbot.\n",
            "\n",
            "ðŸ§‘ You: my favorite color is blue\n",
            "ðŸ¤– Bot: It's great to know that your favorite color is blue! Blue often represents calmness and serenity, but it can also symbolize depth and stability. Is there anything specific about the color blue you'd like to discuss or explore? If you have any other questions or need assistance with something else, feel free to ask!\n",
            "\n",
            "If this was in response to a conversation context about preferences or personal details, please let me know if there's more you'd like to share or delve into regarding that.\n",
            "\n",
            "ðŸ§‘ You: whats my favorite color?\n",
            "ðŸ¤– Bot: Your favorite color is blue! Blue often represents calmness, serenity, depth, and stability. If there's anything specific you'd like to discuss or explore about this color, feel free to let me know. If you have any other questions or need assistance with something else, I'm here to help!\n",
            "\n",
            "\n",
            "\n",
            "ðŸ§‘ You: summarize the paper in 5 bullet points\n",
            "ðŸ¤– Bot: Certainly! Based on the context provided, here's a structured summary of the paper in five bullet points:\n",
            "\n",
            "1. **Key Findings/Main Arguments**:\n",
            "   - The paper focuses on providing an interpolation bound for attention scores defined as \\(a(s) = \\text{Re} \\left( \\sum_{j=0}^{Pd/2-1} h_j e^{i\\theta_j s} \\right)\\), where \\(\\theta_j = c^{-2j/d}\\).\n",
            "   - It establishes that the interpolation error between the actual value \\(a(s)\\) and its linear approximation \\(a_\\text{linear}(s)\\) is bounded by a specific formula.\n",
            "\n",
            "2. **Significant Methodologies**:\n",
            "   - The paper uses Taylor expansion to derive expressions for approximating attention scores at points \\(s_1\\) and \\(s_2\\), leading to an expression for the interpolation error.\n",
            "   - Linear interpolation between grid points known from LLM pre-training is used, with \\(\\lambda(s)\\) facilitating this.\n",
            "\n",
            "3. **Implications/Applications**:\n",
            "   - The findings suggest a way to control or predict errors in attention scores when interpolating values within defined ranges.\n",
            "   - This has implications for improving the robustness and accuracy of models that rely on such interpolation, particularly in large language models (LLMs).\n",
            "\n",
            "4. **Limitations/Further Research**:\n",
            "   - While the paper provides bounds for interpolation error, it assumes specific behaviors based on pre-training conditions which might not generalize to all settings.\n",
            "   - Further research could explore how these bounds hold under different model configurations or training regimes.\n",
            "\n",
            "5. **Contribution to Field**:\n",
            "   - The paper contributes by providing a theoretical framework that quantifies the errors associated with linear interpolation of attention scores in LLMs, an area where empirical guidelines are often lacking.\n",
            "   - It builds on existing literature around LLM performance and stability, offering insights into how model pre-training can influence downstream tasks through interpolation accuracy.\n",
            "\n",
            "This summary encapsulates the essence of the paper based on the context provided. If there's anything more specific you'd like to explore or if there's another question you have, feel free to ask!\n",
            "\n",
            "ðŸ§‘ You: quit\n",
            "ðŸ‘‹ Bye!\n"
          ]
        }
      ]
    }
  ]
}